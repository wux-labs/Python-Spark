{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a08d243a-b4ea-45b9-9898-9f7ccc6a63eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DStream入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa2dfa28-30db-4312-9307-185f7a609620",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 基本数据源\n",
    "\n",
    "直接在流式处理上下文API中提供的源。示例：文件系统和套接字连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a93f5d62-059e-4ea0-a562-31f52a4f3db4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 文件流\n",
    "\n",
    "为了从与HDFS API兼容的任何文件系统（即 HDFS、S3、NFS 等）上的文件读取数据，可以通过以下方式创建 DStream。\n",
    "\n",
    "```\n",
    "StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)\n",
    "```\n",
    "\n",
    "文件流不需要运行接收器，因此无需分配任何内核来接收文件数据。\n",
    "\n",
    "对于简单的文本文件，最简单的方法是：StreamingContext.textFileStream(dataDirectory)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1be3adb3-02ee-4aeb-82c6-87e7c9d30b01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "\n",
    "import org.apache.hadoop.io.Text\n",
    "import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10))\n",
    "\n",
    "val lines = ssc.fileStream[Text,Text,KeyValueTextInputFormat](\"/mnt/databrickscontainer1/SparkStreaming\")\n",
    "\n",
    "// 由于fileStream读取到的数据是KV型数据，我们只处理其中的K值，所以需要用 _._1 来取得K值\n",
    "val words = lines.flatMap(x => x._1.toString().split(\" \")).map((_,1)).reduceByKey(_+_)\n",
    "\n",
    "words.print()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "// ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5698a2d6-4e27-45ff-8e31-b94d440f5c86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**由于fileStream方法需要泛型支持，所以fileStream方法在Python中不可用，Python仅支持textFileStream。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc6e1339-0a0f-41df-a04e-e460b5185b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "lines = ssc.textFileStream(\"/mnt/databrickscontainer1/SparkStreaming\")\n",
    "\n",
    "words = lines.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda a,b: a + b)\n",
    "\n",
    "words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "671e0bdf-b0b7-469c-9a33-25d2b0a7a702",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b79e4f23-1dc4-4401-b907-6cb696d37ef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 如何监视目录\n",
    "\n",
    "Spark流式处理将监视dataDirectory目录并处理在该目录中创建的任何文件。\n",
    "\n",
    "* 可以监视一个简单的目录，例如 \"hdfs://namenode:8040/logs/\"。直接位于此路径下的所有文件都将在被发现时进行处理。\n",
    "* 可以提供 POSIX glob 模式，例如 \"hdfs://namenode:8040/logs/2017/\\*\" 。DStream将包含目录中与模式匹配的所有文件。也就是说：它是目录的模式，而不是目录中的文件的模式。\n",
    "* 所有文件必须采用相同的数据格式。\n",
    "* 文件根据其修改时间（而不是其创建时间）被视为时间段的一部分。\n",
    "* 处理后，在当前窗口中对文件所做的更改将不会导致重新读取该文件。也就是说：更新将被忽略。\n",
    "* 目录下的文件越多，扫描更改所需的时间就越长，即使没有修改任何文件也是如此。\n",
    "* 如果使用通配符来标识目录（例如 \"hdfs://namenode:8040/logs/2016-\\*\"），则重命名整个目录以匹配路径会将该目录添加到受监视目录列表中。只有目录中修改时间在当前窗口内的文件才会包含在流中。\n",
    "* 调用 FileSystem.setTimes() 来修复时间戳是一种在以后的窗口中选取文件的方法，即使其内容没有更改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "292a1080-ed11-42fc-ac87-c2819384b654",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 使用对象存储作为数据源\n",
    "\n",
    "“Full”文件系统（如 HDFS）倾向于在创建输出流后立即设置其文件的修改时间。当文件被打开时，甚至在数据完全写入之前，它也可能包含在DStream中，之后将忽略同一窗口中对文件的更新。也就是说：可能会错过更改，并且从流中省略数据。\n",
    "\n",
    "要确保在窗口中选取更改，请将文件写入不受监视的目录，然后在输出流关闭后立即将其重命名为目标目录。如果重命名的文件在创建过程中出现在扫描的目标目录中，则将选取新数据。\n",
    "\n",
    "相比之下，对象存储（如Amazon S3和Azure存储）通常具有**缓慢的重命名操作**，因为数据实际上是复制的。此外，重命名的对象可能将rename()操作的时间作为其修改时间，因此可能不被视为原始创建时间的窗口的一部分，也就是有延迟。\n",
    "\n",
    "需要针对目标对象存储进行仔细测试，以验证存储的时间戳行为是否与Spark流式处理的预期一致。直接写入目标目录可能是通过所选对象存储流式传输数据的适当策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7969d7ff-8bb2-4b08-8617-39d4033eed52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 套接字\n",
    "\n",
    "可以使用 ssc.socketTextStream(...) 从通过TCP套接字连接接收的文本数据创建DStream。\n",
    "\n",
    "> nc -lk 5555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2551716-8448-4b45-95a4-a8cc727a62dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# lines = ssc.socketTextStream(\"localhost\", 5555)\n",
    "lines = ssc.socketTextStream(\"20.187.125.128\", 5555)\n",
    "\n",
    "words = lines.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda a,b: a + b)\n",
    "\n",
    "words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9594010-562b-4fb5-aa2e-3c18223cfbd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4ec45a7-6865-4576-80e1-36b06196ac89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### RDD队列\n",
    "\n",
    "要使用测试数据测试Spark流式处理应用程序，还可以使用streamingContext.queueStream(queueOfRDDs)创建基于RDD队列的DStream。推送到队列中的每个RDD将被视为DStream中的一批数据，并像流一样进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18a8d606-d58c-41a1-8de0-118089f45ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue = [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "# 往队列中写入数据\n",
    "for i in range(5):\n",
    "    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "words = lines.map(lambda x: (x % 10, 1)).reduceByKey(lambda a,b: a + b)\n",
    "\n",
    "words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd531635-703f-4162-97b9-f5419ad250e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "baadde37-1785-4286-9957-f009aceb2ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 高级数据源\n",
    "\n",
    "Spark Streaming支持从外部数据源获取数据来构建DStream。实际项目中使用较多的就是Kafka。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d59937ba-1bcf-48f4-b0b9-94b5de3ccf49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Kafka\n",
    "\n",
    "在Spark 2.x版本中，PySpark Streaming还支持Kafka模块，但是在Spark 3.x版本中，该功能被移除了。\n",
    "\n",
    "https://spark.apache.org/docs/2.4.8/api/python/index.html\n",
    "\n",
    "https://spark.apache.org/docs/3.0.0/api/python/index.html\n",
    "\n",
    "https://spark.apache.org/docs/3.2.1/api/python/index.html\n",
    "\n",
    "我们无法直接使用Python来开发，只能使用Scala/Java。\n",
    "\n",
    "需要为集群安装库：`org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1`和`org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0fb7131-a29d-44c2-9f86-cab95b9e9a87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ddaee33-e725-40f1-a81e-b006a6c78163",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Ubuntu20.04下Kafka安装与部署\n",
    "\n",
    "```\n",
    "sudo apt-get update\n",
    "```\n",
    "\n",
    "##### 安装JDK\n",
    "\n",
    "```\n",
    "sudo apt install openjdk-8-jdk\n",
    "```\n",
    "\n",
    "##### 下载Kafka\n",
    "\n",
    "```\n",
    "wget https://downloads.apache.org/kafka/3.3.1/kafka_2.12-3.3.1.tgz\n",
    "wget https://archive.apache.org/dist/kafka/3.1.0/kafka_2.12-3.1.0.tgz\n",
    "```\n",
    "\n",
    "##### 安装Kafka\n",
    "\n",
    "###### 解压\n",
    "\n",
    "```\n",
    "mkdir apps\n",
    "tar -xzf kafka_2.12-3.1.0.tgz -C apps/\n",
    "```\n",
    "\n",
    "###### 配置\n",
    "\n",
    "```\n",
    "vi apps/kafka_2.12-3.1.0/config/zookeeper.properties\n",
    "vi apps/kafka_2.12-3.1.0/config/server.properties\n",
    "```\n",
    "\n",
    "###### 启动\n",
    "\n",
    "```\n",
    "cd apps/kafka_2.12-3.1.0\n",
    "\n",
    "bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n",
    "\n",
    "bin/kafka-server-start.sh -daemon config/server.properties\n",
    "```\n",
    "\n",
    "##### 使用Kafka\n",
    "\n",
    "###### 创建Topic\n",
    "\n",
    "```\n",
    "bin/kafka-topics.sh --create --topic KafkaFirstTopic --partitions 1 --replication-factor 1 --bootstrap-server 10.0.0.4:9092\n",
    "\n",
    "bin/kafka-topics.sh --list --bootstrap-server 10.0.0.4:9092\n",
    "```\n",
    "\n",
    "###### 发送消息\n",
    "\n",
    "```\n",
    "bin/kafka-console-producer.sh --bootstrap-server 10.0.0.4:9092 --topic KafkaFirstTopic\n",
    "```\n",
    "\n",
    "###### 消费消息\n",
    "\n",
    "```\n",
    "bin/kafka-console-consumer.sh --bootstrap-server 10.0.0.4:9092 --topic KafkaFirstTopic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "146f98ff-0fa4-4ab8-aff9-f8a45ecbc514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "ping 104.208.105.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98863f0c-c48f-4af5-94a4-cbbe503be891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "cat /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d609cbd6-1019-43c8-85c7-98657a9218b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "echo \"104.208.105.98 wux-labs-vm.internal.cloudapp.net\" >> /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3bb064ee-acbe-4b76-bce8-288af522ad96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "import org.apache.spark.streaming.kafka010._\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10))\n",
    "\n",
    "val kafkaParams = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> \"104.208.105.98:9092\",\n",
    "  \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"databricks_kafka_group\",\n",
    "  \"auto.offset.reset\" -> \"latest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "val topics = Array(\"KafkaFirstTopic\")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  PreferConsistent,\n",
    "  Subscribe[String, String](topics, kafkaParams)\n",
    ")\n",
    "\n",
    "// stream.map(record => (record.key, record.value)).reduceByKey(_+_).print()\n",
    "stream.flatMap(x => x.value.toString().split(\" \")).map((_,1)).reduceByKey(_+_).print()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ca393a3-c72d-49f7-921e-1bb9387720fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d165cb3d-92b6-459e-a216-e1f92d50a46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36421b31-9b65-400b-b928-a8aecbe776fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "kill -9 516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b5538bd-3b75-448c-97e7-a51a516b611d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DStream上的Transformations\n",
    "\n",
    "与RDD类似，转换允许修改输入DStream中的数据。DStreams支持普通Spark RDD上可用的许多转换。一些常见的如下。\n",
    "\n",
    "| Transformation                           | Meaning                                                      |\n",
    "| :--------------------------------------- | :----------------------------------------------------------- |\n",
    "| **map**(*func*)                          | Return a new DStream by passing each element of the source DStream through a function *func*. |\n",
    "| **flatMap**(*func*)                      | Similar to map, but each input item can be mapped to 0 or more output items. |\n",
    "| **filter**(*func*)                       | Return a new DStream by selecting only the records of the source DStream on which *func* returns true. |\n",
    "| **repartition**(*numPartitions*)         | Changes the level of parallelism in this DStream by creating more or fewer partitions. |\n",
    "| **union**(*otherStream*)                 | Return a new DStream that contains the union of the elements in the source DStream and *otherDStream*. |\n",
    "| **count**()                              | Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream. |\n",
    "| **reduce**(*func*)                       | Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function *func* (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel. |\n",
    "| **countByValue**()                       | When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream. |\n",
    "| **reduceByKey**(*func*, [*numTasks*])    | When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. **Note:** By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property `spark.default.parallelism`) to do the grouping. You can pass an optional `numTasks` argument to set a different number of tasks. |\n",
    "| **join**(*otherStream*, [*numTasks*])    | When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key. |\n",
    "| **cogroup**(*otherStream*, [*numTasks*]) | When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples. |\n",
    "| **transform**(*func*)                    | Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream. |\n",
    "| **updateStateByKey**(*func*)             | Return a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a608debb-d369-496a-8cfc-0f83d3813fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### map\n",
    "\n",
    "是将DStream的数据一条一条处理，处理的逻辑是基于map算子中接收的处理函数的，返回新的DStream（TransformedDStream）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e59d7d7c-177b-465b-9dbf-8a8312192109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue = [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "# 往队列中写入数据\n",
    "for i in range(5):\n",
    "    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# DStream\n",
    "print(type(lines))\n",
    "\n",
    "words = lines.map(lambda x: (x % 10, 1))\n",
    "\n",
    "# TransformedDStream\n",
    "print(type(words))\n",
    "words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43ce66cd-1ccc-4e51-9cb4-c08a32c59609",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "07dd910b-cb53-4891-b7ec-580dac57fa63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### filter\n",
    "\n",
    "筛选满足条件的数据，返回新的DStream（TransformedDStream）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c288949-bdbb-430d-87cc-be0e56520cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue = [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "# 往队列中写入数据\n",
    "for i in range(5):\n",
    "    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# DStream\n",
    "print(type(lines))\n",
    "\n",
    "words = lines.filter(lambda x: x % 5 == 0)\n",
    "\n",
    "# TransformedDStream\n",
    "print(type(words))\n",
    "words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c684e51-3bb9-4d1a-bf93-3d5c5566eee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b9a9a19-0596-446d-b3c4-bc4a22105283",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### reduceByKey\n",
    "\n",
    "针对K-V型DStream，自动按照K分组，然后根据提供的聚合逻辑，完成组内数据的聚合操作，返回新的DStream（TransformedDStream）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0585c285-92b9-411d-90fb-a4544bbde2db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue = [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "# 往队列中写入数据\n",
    "for i in range(5):\n",
    "    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "words = lines.map(lambda x: (x % 10, 1)).reduceByKey(lambda a,b: a + b)\n",
    "\n",
    "words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89f1d88b-55e0-42e4-9944-16e731ef875f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6aeb3717-32b9-463e-8d10-577139b2b584",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### updateStateByKey\n",
    "\n",
    "updateStateByKey操作允许您保持任意状态，同时不断使用新信息对其进行更新，这需要chekcpoint保存历史数据。要使用它，您必须执行两个步骤。\n",
    "* 定义状态 - 状态可以是任意数据类型。\n",
    "* 定义状态更新函数 - 使用函数指定如何使用以前的状态和输入流中的新值来更新状态。\n",
    "\n",
    "在每个批次中，Spark都会对所有现有key应用状态更新功能，无论它们是否在批处理中具有新数据。如果更新函数返回None，则键值对将被淘汰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e77a8377-4972-4551-9942-0a2e29792c91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    if len(newValues) == 2:\n",
    "        return None\n",
    "    return sum(newValues, runningCount)\n",
    "\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "lines = ssc.textFileStream(\"/mnt/databrickscontainer1/SparkStreaming\")\n",
    "\n",
    "# 按行读取文件、用空格拆分单词\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "# 为每个单词计数为1\n",
    "pairs = words.map(lambda x: (x, 1))\n",
    "# 根据单词汇总单词的个数\n",
    "counts = pairs.updateStateByKey(updateFunction)\n",
    "# 打印最终的结果，得到WordCount\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5593c4f8-882f-4d41-85ef-3fa6e71def7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c89ac536-1962-4ef4-b516-50c01e7379fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### join\n",
    "\n",
    "对两个RDD执行JOIN操作（可实现SQL的内、外连接）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "132eb6b3-0dd3-4c3f-804a-c2d78ed51a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % (i * j),i * j) for j in range(1, 11)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue2 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue2 += [ssc.sparkContext.parallelize([(\"value%s\" % (i * j),i * j) for j in range(6, 16)])]\n",
    "    \n",
    "rdd2 = ssc.queueStream(rddQueue2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "369af17e-4701-492d-9455-0cc17607ab90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 单个流执行没有问题\n",
    "rdd1.pprint()\n",
    "rdd2.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0eb76a10-818e-44f3-a880-7eba6fc00ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10bdff68-052f-4811-b978-ed37bf9bda97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2cd9e05-5026-43e8-8454-5c81e5a7fecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1.join(rdd2).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de873a15-1d39-4084-9539-4c30ebee560c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3af58255-22ee-446d-a332-36fa12db68bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1.leftOuterJoin(rdd2).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "998b6bab-0c32-4025-ba50-875ba961f776",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### right outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed41df3b-3100-4708-a16f-f2b23bb48d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1.rightOuterJoin(rdd2).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec4ade07-ed15-4ec0-a7b9-23a7da0a0e99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10a85ca3-9379-4951-8451-a5fc7d4b69ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1.fullOuterJoin(rdd2).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c72abd39-c9e6-4608-a4b1-0a6f9fdff563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 存在一个小问题\n",
    "\n",
    "DStream不能直接与普通的RDD进行Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "efd03487-ab8e-41c9-a540-e598f14147b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(rdd))\n",
    "print(type(rdd1))\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35f1a234-fdf6-4cd9-b8ee-f652d80d47e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 'RDD' object has no attribute '_jdstream'\n",
    "rdd1.join(rdd).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c5cae11-1a84-43ff-9f1c-45d3b739a93a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### transform\n",
    "\n",
    "transform操作允许在DStream上应用任意 RDD 到 RDD 的函数。它可以用于处理没有在DStream API公布的RDD操作，这提供了很大的灵活性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca899733-afaa-4277-adf5-693e09d4b821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % (i * j),i * j) for j in range(1, 11)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "# 通过transform将DStream中的元素与RDD进行join\n",
    "rdd1.transform(lambda rd1: rd1.fullOuterJoin(rdd)).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "93aa3ef3-c353-4c4c-8a04-6bbbfa9f96f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "029d9e45-536a-4da2-9b95-9a3bb7b463ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### window\n",
    "\n",
    "Spark Streaming还提供窗口计算，允许通过滑动窗口对数据进行转换。\n",
    "\n",
    "![](https://spark.apache.org/docs/3.2.1/img/streaming-dstream-window.png)\n",
    "\n",
    "每次窗口在源DStream上滑动时，落在窗口内的源RDD都会被组合并对其进行操作，以生成窗口DStream的RDD。\n",
    "\n",
    "任何窗口操作都需要指定两个参数。\n",
    "\n",
    "* window length - 窗口的持续时间（图中为 3）。\n",
    "* sliding interval - 执行窗口操作的间隔（图中为 2）。\n",
    "\n",
    ">这两个参数必须是源DStream的批处理间隔（BatchDuration）的倍数\n",
    "\n",
    "| Transformation                                               | Meaning                                                      |\n",
    "| :----------------------------------------------------------- | :----------------------------------------------------------- |\n",
    "| **window**(*windowLength*, *slideInterval*)                  | Return a new DStream which is computed based on windowed batches of the source DStream. |\n",
    "| **countByWindow**(*windowLength*, *slideInterval*)           | Return a sliding window count of elements in the stream.     |\n",
    "| **reduceByWindow**(*func*, *windowLength*, *slideInterval*)  | Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using *func*. The function should be associative and commutative so that it can be computed correctly in parallel. |\n",
    "| **reduceByKeyAndWindow**(*func*, *windowLength*, *slideInterval*, [*numTasks*]) | When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function *func* over batches in a sliding window. **Note:** By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property `spark.default.parallelism`) to do the grouping. You can pass an optional `numTasks` argument to set a different number of tasks. |\n",
    "| **reduceByKeyAndWindow**(*func*, *invFunc*, *windowLength*, *slideInterval*, [*numTasks*]) | A more efficient version of the above `reduceByKeyAndWindow()` where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter *invFunc*). Like in `reduceByKeyAndWindow`, the number of reduce tasks is configurable through an optional argument. Note that [checkpointing](https://spark.apache.org/docs/3.2.1/streaming-programming-guide.html#checkpointing) must be enabled for using this operation. |\n",
    "| **countByValueAndWindow**(*windowLength*, *slideInterval*, [*numTasks*]) | When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in `reduceByKeyAndWindow`, the number of reduce tasks is configurable through an optional argument. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e25f71d5-f4ba-410c-bd2a-16382273de8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### countByWindow\n",
    "\n",
    "返回流中元素的滑动窗口中元素的个数。\n",
    "\n",
    "参数：\n",
    "* windowLength  - 窗口的持续时间\n",
    "* slideInterval - 执行窗口操作的间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34f56f77-9215-4f9a-9cb3-640d171f957c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % (i * j),i * j) for j in range(1, 11)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "# rdd1.count().pprint()\n",
    "# rdd1.countByWindow(Seconds(30), Seconds(10)).pprint()\n",
    "rdd1.countByWindow(30, 10).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d3955b9-65d1-4f83-8935-39de964b3dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8e6678ce-e614-4953-af43-53b2f8677183",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### reduceByKeyAndWindow\n",
    "\n",
    "当在(K,V)型的DStream上调用时，返回一个新的(K,V)型的DStream，其中每个键的值在滑动窗口中使用给定的reduce函数func对批次进行聚合。\n",
    "\n",
    "几个重要的参数：\n",
    "\n",
    "* func - 聚合函数\n",
    "* invFunc - 反向函数（就是用于处理从上一个窗口到当前窗口的过程中从窗口中移除出去的数据的）\n",
    "* windowLength  - 窗口的持续时间\n",
    "* slideInterval - 执行窗口操作的间隔\n",
    "* [numTasks]\n",
    "\n",
    "> 注意：  \n",
    "> 默认情况下，这使用Spark的默认并行任务数（本地模式为2，集群模式下由配置属性Spark.default.parallelism确定）进行分组。可以传递可选的numTasks参数来设置不同数量的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7d871356-f6fc-4ca0-bc3e-b81b1e98a424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "# rdd1.reduceByKey(lambda a,b: a + b).pprint()\n",
    "\n",
    "# rdd1.reduceByKeyAndWindow(lambda a, b: a + b, Seconds(30), Seconds(10)).pprint()\n",
    "# rdd1.reduceByKeyAndWindow(lambda a, b: a + b, 30, 10).pprint()\n",
    "\n",
    "# rdd1.reduceByKeyAndWindow(lambda a, b: a + b, lambda x, y: x - y, Seconds(30), Seconds(10)).pprint()\n",
    "rdd1.reduceByKeyAndWindow(lambda a, b: a + b, lambda x, y: x - y, 30, 10).pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ca764de-3995-4554-866a-a5142e6efac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4363810a-009f-4902-9fa0-17b91035e534",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DStream上的输出操作\n",
    "\n",
    "输出操作允许将DStream的数据推送到外部系统，比如数据库或文件系统。由于输出操作实际上允许外部系统使用转换后的数据，因此它们会触发执行所有DStream转换（类似于RDD的Action）。\n",
    "\n",
    "| Output Operation                            | Meaning                                                      |\n",
    "| :------------------------------------------ | :----------------------------------------------------------- |\n",
    "| **print**()                                 | Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. **Python API** This is called **pprint()** in the Python API. |\n",
    "| **saveAsTextFiles**(*prefix*, [*suffix*])   | Save this DStream's contents as text files. The file name at each batch interval is generated based on *prefix* and *suffix*: *\"prefix-TIME_IN_MS[.suffix]\"*. |\n",
    "| **saveAsObjectFiles**(*prefix*, [*suffix*]) | Save this DStream's contents as `SequenceFiles` of serialized Java objects. The file name at each batch interval is generated based on *prefix* and *suffix*: *\"prefix-TIME_IN_MS[.suffix]\"*. **Python API** This is not available in the Python API. |\n",
    "| **saveAsHadoopFiles**(*prefix*, [*suffix*]) | Save this DStream's contents as Hadoop files. The file name at each batch interval is generated based on *prefix* and *suffix*: *\"prefix-TIME_IN_MS[.suffix]\"*. **Python API** This is not available in the Python API. |\n",
    "| **foreachRDD**(*func*)                      | The most generic output operator that applies a function, *func*, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function *func* is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6c44e8c-49ad-4727-bec2-159a3cf09490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### print / pprint\n",
    "\n",
    "打印DStream的前10条数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18cf8be5-a248-4337-9a20-2e07376c3cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 101)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "rdd1.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2fd3fc40-d790-4043-bea3-fea6a0c35754",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18af79c8-7d68-40bb-8220-f70be7c13adc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### saveAsTextFiles\n",
    "\n",
    "将DStream的数据保存到文本文件，每个批量间隔的数据保存一个独立的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "995ee859-88df-45bd-b529-19b4c4a46ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 101)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "rdd1.saveAsTextFiles('/mnt/databrickscontainer1/SparkStreaming/output','dat')\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47762743-fd87-47f1-be48-dd5b344488f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fce5df3-2b44-4346-b8d9-edfcccce32ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### foreachRDD\n",
    "\n",
    "foreachRDD是一个功能强大的原语，允许将数据发送到外部系统。但是，了解如何正确有效地使用此基元非常重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce40e9db-3223-45a7-9341-f83c6c4f2f4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 101)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "rdd1.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: print(list(partition))))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7d378a52-35cf-4ea5-bf9b-388ec0210c0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8adc6a45-dd42-4a2b-9024-6c8dcdc61f30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 注意\n",
    "\n",
    "官方文档提到：了解如何正确有效地使用此基元非常重要。\n",
    "\n",
    "> [Design Patterns for using foreachRDD](https://spark.apache.org/docs/3.2.1/streaming-programming-guide.html#design-patterns-for-using-foreachrdd)  \n",
    ">  However, it is important to understand how to use this primitive correctly and efficiently.\n",
    "\n",
    "当需要将DStream的数据推送到外部系统时，比如关系型数据库，如果需要创建数据源连接：\n",
    "* 不要在Driver端创建连接，因为可能出现序列化反序列化失败的问题\n",
    "* 不要为每个RDD的数据创建一个连接，一方面会导致连接数过多，一方面创建连接的开销很大\n",
    "* 尽量让一次连接处理一个分区的数据\n",
    "* 可以使用共享连接池"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e6af7b0-6939-44f4-bf1e-e797b0900ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DStream上的SQL操作\n",
    "\n",
    "要想执行SQL操作，需要将DStream中的RDD注册成DataFrame/视图，然后才能进行SQL操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f421b35-a244-4e83-a539-ee615603f580",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(\"/mnt/databrickscontainer1/checkpoint/\")\n",
    "\n",
    "rdd = ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 11)])\n",
    "\n",
    "# 定义一个队列\n",
    "rddQueue1 = [rdd]\n",
    "\n",
    "for i in range(2,10):\n",
    "    rddQueue1 += [ssc.sparkContext.parallelize([(\"value%s\" % j, j) for j in range(1, 101)])]\n",
    "    \n",
    "rdd1 = ssc.queueStream(rddQueue1)\n",
    "\n",
    "# SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "def processRDD(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    print(type(rdd))\n",
    "    \n",
    "    # SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n",
    "    sparki = getSparkSessionInstance(rdd.context.getConf())\n",
    "    \n",
    "    # 通过RDD创建DataFrame方式1\n",
    "    # SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n",
    "    # df = spark.createDataFrame(rdd, schema=[\"word\",\"value\"])\n",
    "    df = sparki.createDataFrame(rdd, schema=[\"word\",\"value\"])\n",
    "    \n",
    "    # 通过RDD创建DataFrame方式3\n",
    "    schema = StructType().add(\"word\", StringType(), nullable=False).add(\"value\", IntegerType(), nullable=False)\n",
    "    df = rdd.toDF(schema)\n",
    "\n",
    "    df.createOrReplaceTempView(\"dstream_table\")\n",
    "    sparki.sql(\"select * from dstream_table\").show()\n",
    "\n",
    "rdd1.foreachRDD(processRDD)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a3bb120-e2b0-4ac8-bee5-f105a487332d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a29d0b4-896c-40f8-8ffc-9cfa096782a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DStream上的持久化\n",
    "\n",
    "在学习SparkCore的时候，我们知道RDD的数据是过程数据，在下一次要用到RDD的数据的时候，再根据血缘关系，从头重新处理一遍RDD的数据。RDD提供了`cache`、`persist`、`checkpoint`来进行数据的持久化。\n",
    "\n",
    "与RDD类似，DStream还允许开发人员将流的数据保存在内存中。\n",
    "\n",
    "对于基于窗口的操作（如reduceByWindow和reduceByKeyAndWindow）和基于状态的操作（如updateStateByKey），这是隐式的。因此，由基于窗口的操作生成的DStream会自动保留在内存中，而无需开发人员调用persist()。\n",
    "\n",
    "> 与RDD不同，DStream的默认持久性级别将数据序列化在内存中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82269f41-b369-4a47-9726-ddfbbef462a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 检查点/Checkpoint\n",
    "\n",
    "流应用程序必须全天候运行，因此必须能够灵活应对与应用程序逻辑无关的故障（例如，系统故障、JVM 崩溃等）。为了实现这一点，Spark Streaming需要将足够多的信息检查点发送到容错存储系统，以便它可以从故障中恢复。\n",
    "\n",
    "有两种类型的数据是检查点的。\n",
    "\n",
    "* 元数据检查点 - 将定义流计算的信息保存到HDFS等容错存储中。这用于从运行流式处理应用程序驱动程序的节点的故障中恢复。元数据包括：\n",
    "  * 配置 - 用于创建流式处理应用程序的配置。\n",
    "  * DStream操作 - 定义流式处理应用程序的DStream操作集。\n",
    "  * 未完成的批次 - 作业已排队但尚未完成的批次。\n",
    "* 数据检查点 - 将生成的RDD保存到可靠的存储。在某些跨多个批次合并数据的有状态转换中，这是必需的。在此类转换中，生成的RDD依赖于先前批次的RDD，这会导致依赖关系链的长度随着时间的推移而不断增加。为了避免恢复时间的这种无限增加（与依赖关系链成正比），有状态转换的中间RDD定期检查点到可靠存储（例如HDFS）以切断依赖链。\n",
    "\n",
    "总而言之，元数据检查点主要用于从驱动程序故障中恢复，而数据或RDD检查点对于基本功能（如果使用有状态转换）也是必需的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c9f4292-1414-4848-9af1-c4851a16cc02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 何时启用检查点\n",
    "必须为具有以下任何要求的应用程序启用检查点：\n",
    "\n",
    "* 有状态转换的使用 - 如果在应用程序中使用了updateStateByKey或reduceByKeyAndWindow（具有反函数），则必须提供检查点目录以允许定期RDD检查点。\n",
    "* 从运行应用程序的驱动程序的故障中恢复 - 元数据检查点用于恢复进度信息。\n",
    "\n",
    "> 请注意，无需启用检查点即可运行没有上述有状态转换的简单流式处理应用程序。在这种情况下，从驱动程序故障中恢复也将是部分的（某些已接收但未处理的数据可能会丢失）。这通常是可以接受的，许多人以这种方式运行Spark Streaming应用程序。对非Hadoop环境的支持有望在未来得到改善。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae832c39-794e-45b6-b9c2-deeb0893b1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 如何配置检查点\n",
    "\n",
    "可以通过在容错、可靠的文件系统（例如，HDFS、S3等）中设置一个目录来启用检查点操作，检查点信息将保存到该目录。这是通过使用streamingContext.checkpoint(checkpointDirectory)来完成的。这将允许您使用上述有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流式处理应用程序以具有以下行为。\n",
    "\n",
    "* 当程序首次启动时，它将创建一个新的StreamingContext，设置所有流，然后调用start()。\n",
    "* 当程序在失败后重新启动时，它将从检查点目录中的检查点数据重新创建流文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9947eff5-216d-4914-9d6e-3fbfd7bcbff8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 累加器、广播变量和检查点\n",
    "\n",
    "累加器和广播变量无法从 Spark 流式处理中的检查点恢复。如果启用检查点并同时使用累加器或广播变量，则必须为累加器和广播变量创建延迟实例化的单一实例，以便在驱动程序失败时重新启动后可以重新实例化它们。\n",
    "\n",
    "请参阅完整的[源代码](https://github.com/apache/spark/blob/v3.2.1/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala)。"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2180139033494661,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DStream入门",
   "notebookOrigID": 2180139033493939,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
