{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bae19665-6f5e-48e6-8d79-7a5adb031949",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DataFrame入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b186604d-b6cd-40b1-836e-cfa49f6b6238",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "环境初始化\n",
    "> 首先执行环境的初始化。  \n",
    "> 将存储账户与Spark环境关联，以便于在Spark程序中可以使用存储。  \n",
    "> `dfs_endpoint` 是文件系统的根端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97cd776d-3997-4108-a8c6-28be2a591c08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../../initialization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8506f7d-0105-44b8-acb9-76697cb2d7a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的组成\n",
    "\n",
    "DataFrame是一个二维表结构，那么表结构就应该有：\n",
    "* 行\n",
    "* 列\n",
    "* 表结构描述\n",
    "\n",
    "基于这样的前提，DataFrame的组成如下。\n",
    "\n",
    "在结构层面：\n",
    "* StructType对象描述整个DataFrame的表结构\n",
    "* StructField对象描述一个列的信息\n",
    "\n",
    "在数据层面：\n",
    "* Row对象记录一行数据\n",
    "* Column对象记录一列数据并包含列信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77426e5f-0f6a-4b44-9bbc-e1d29f5784e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "# DataFrame 的结构\n",
    "df.printSchema()\n",
    "\n",
    "# DataFrame的行 Row\n",
    "row = df.head()\n",
    "print(row)\n",
    "\n",
    "# DataFrame的列 Column\n",
    "col = df[0]\n",
    "print(col)\n",
    "print(col.name(), col.desc(), col.isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64dc8e26-d9b8-4352-a2cf-849ce0e9e428",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "一个StructField记录了：**列名、列类型、列是否允许为空**。\n",
    "\n",
    "多个StructField组成一个StructType对象。\n",
    "\n",
    "一个StructType对象可以描述一个DataFrame有几个列、每个列的名字、每个列的类型、每个列是否允许为空。\n",
    "\n",
    "一个Row对象描述一行数据，比如：Row(Order Number='16118', Order Date='03/08/2019 20:25', Item Name='Plain Papadum', Quantity='2', Product Price='0.8', Total products='6')\n",
    "\n",
    "一个Column对象描述一列数据，Column对象包含一列数据和列的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "160a5bda-a22b-4632-9087-51e88cd39de0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的代码构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dff1cec1-be3b-4aa7-b5d5-bb77bbf4d46f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 基于RDD方式1\n",
    "\n",
    "DataFrame对象可以从RDD转换而来，都是分布式数据集其实就是转换一下内部存储的结构，转换为二维表结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "651204a6-1c7e-4b60-9755-1cf9538f0daa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\").map(lambda x: x.split(\",\"))\n",
    "\n",
    "print(type(rdd))\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema=[\"OrderNumber1\",\"OrderDate1\",\"ItemName1\",\"Quantity1\",\"ProductPrice1\",\"TotalProducts1\"])\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03557e15-3674-4c13-8d00-9af3224a3440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 基于RDD方式2\n",
    "\n",
    "通过StructType对象来定义DataFrame的“表结构”转换RDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c35a27e-50a0-4bba-8ed6-8a0bd8dfda03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "rdd = sc.textFile(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\").map(lambda x: x.split(\",\")).filter(lambda x: x[0] != \"Order Number\").map(lambda x: (x[0],x[1],x[2],int(x[3]),float(x[4]),float(x[5])))\n",
    "\n",
    "print(type(rdd))\n",
    "\n",
    "schema = StructType().\\\n",
    "add(\"OrderNumber2\", StringType(), nullable=False).\\\n",
    "add(\"OrderDate2\", StringType(), nullable=False).\\\n",
    "add(\"ItemName2\", StringType(), nullable=False).\\\n",
    "add(\"Quantity2\", IntegerType(), nullable=False).\\\n",
    "add(\"ProductPrice2\", DoubleType(), nullable=False).\\\n",
    "add(\"TotalProducts2\", DoubleType(), nullable=False)\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0875dabf-aabd-4974-b0b4-04d96dd0d343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 基于RDD方式3\n",
    "\n",
    "使用RDD的toDF方法转换RDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e74fbeb7-2da8-45ea-9abe-9b7bb09ea995",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "rdd = sc.textFile(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\").map(lambda x: x.split(\",\")).filter(lambda x: x[0] != \"Order Number\").map(lambda x: (x[0],x[1],x[2],int(x[3]),float(x[4]),float(x[5])))\n",
    "\n",
    "print(type(rdd))\n",
    "\n",
    "schema = StructType().\\\n",
    "add(\"OrderNumber3\", StringType(), nullable=False).\\\n",
    "add(\"OrderDate3\", StringType(), nullable=False).\\\n",
    "add(\"ItemName3\", StringType(), nullable=False).\\\n",
    "add(\"Quantity3\", IntegerType(), nullable=False).\\\n",
    "add(\"ProductPrice3\", DoubleType(), nullable=False).\\\n",
    "add(\"TotalProducts3\", DoubleType(), nullable=False)\n",
    "\n",
    "df = rdd.toDF(schema)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d581652-09e7-4a51-9c0d-9e6172beef16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 基于Pandas的DataFrame\n",
    "\n",
    "可以将Pandas的DataFrame对象，转变为分布式的SparkSQL的DataFrame对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6275e39-c092-428f-9d37-54b5b26e62fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 通过读取数据文件，得到Pandas的DataFrame\n",
    "pdf = pd.read_csv(\"../../Datasets/restaurant-1-orders.zip\", compression=\"zip\")\n",
    "\n",
    "print(type(pdf))\n",
    "print(pdf.sample(5))\n",
    "\n",
    "# 将Pandas的DataFrame转换成Spark的DataFrame\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "print(type(sdf))\n",
    "sdf.printSchema()\n",
    "sdf.show()\n",
    "\n",
    "display(pdf)\n",
    "display(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c35e28b-49e4-42ea-8309-7df112894edb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 读取外部数据\n",
    "\n",
    "通过SparkSQL的统一API进行数据读取构建DataFrame。\n",
    "\n",
    "支持的外部数据：\n",
    "* text\n",
    "* csv\n",
    "* json\n",
    "* parquet\n",
    "* orc\n",
    "* avro\n",
    "* jdbc\n",
    "* ...\n",
    "\n",
    "统一API示例代码：\n",
    "```\n",
    "spark.read.format(\"text|csv|json|parquet|orc|avro|jdbc|......\")\n",
    ".option(\"K\", \"V\") # option可选\n",
    ".schema(StructType | String) # String的语法如.schema(\"name String\", \"age Int\")\n",
    ".load(\"被读取文件的路径, 支持本地文件系统和HDFS\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e68599d4-5f53-419d-b11d-32197f73fb65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 文本类型数据文件\n",
    "\n",
    "文本类型的数据可以直接用简单的文本编辑器打开进行查看或编辑，比如：text文件、csv文件、json文件等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7935855d-691a-44e0-b874-6b8896f17b65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### text\n",
    "\n",
    "读取text数据源，使用format(\"text\")读取文本数据，读取到的DataFrame只会有一个列，列名默认称之为：value。\n",
    "\n",
    "如果需要将一列进行拆分，则需要使用代码：`.map(lambda x: x.split(\",\"))`来进行拆分转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f2a58117-e0fd-4556-be95-1460ff448a57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"text\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\")\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcdb8621-41bf-4d95-a460-b2e679771747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 read.format(\"text\").load(path) 合并为 read.text(path)\n",
    "spark.read.text(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6b40ff0b-67a8-4b04-8cee-d65f4481ce17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### csv\n",
    "\n",
    "读取csv数据源，使用format(\"csv\")读取csv数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84dadd45-98e7-4a4f-88e3-27cf30725fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\")\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45dfe6e9-cc22-4af3-bd05-bd9b79e4698c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 可以为数据读取指定选项\n",
    "spark.read.format(\"csv\").option(\"header\", True).load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "824dba6d-688f-4a8e-bcad-bc23ed3348ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 read.format(\"csv\").load(path) 合并为 read.csv(path)\n",
    "spark.read.csv(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5041bc82-c9af-4c92-b2d1-2371900ea28c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### json\n",
    "\n",
    "读取json数据源，使用format(\"json\")读取json数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8f2c57b-a0ba-44a0-888c-0f9040cfc9dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 由于没有现成的文件，所以我们只能造一个\n",
    "df = spark.read.csv(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"json\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e4dcff04-dbfb-4869-9220-04ca824ccb76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"/mnt/databrickscontainer1/restaurant-1-orders-json\")\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c31ca2a1-901a-4c4d-ab0b-656382af1a57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 read.format(\"json\").load(path) 合并为 read.json(path)\n",
    "spark.read.json(\"/mnt/databrickscontainer1/restaurant-1-orders-json\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "892529b2-99b3-495e-9bbd-9c567eb7d8ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 自带schema的数据文件\n",
    "\n",
    "在大数据环境中，还有其他各种各样的数据格式，比如：Parquet、Avro、ORC。\n",
    "\n",
    "相同之处：\n",
    "* 基于Hadoop文件系统优化出的存储结构\n",
    "* 提供高效的压缩\n",
    "* 二进制存储格式\n",
    "* 文件可分割，具有很强的伸缩性和并行处理能力\n",
    "* 使用schema进行自我描述\n",
    "* 属于线上格式，可以在Hadoop节点之间传递数据\n",
    "\n",
    "不同之处：\n",
    "* 行式存储or列式存储：Parquet和ORC都以列的形式存储数据，而Avro以基于行的格式存储数据。\n",
    "* 压缩率：基于列的存储区Parquet和ORC提供的压缩率高于基于行的Avro格式。 \n",
    "* 可兼容的平台：\n",
    "  * ORC常用于Hive、Presto\n",
    "  * Parquet常用于Spark、Impala、Drill、Arrow\n",
    "  * Avro常用于Kafka、Druid。\n",
    "\n",
    "这种类型的文件，不能使用简单文本编辑器进行打开查看或编辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed95ec15-df06-4d65-9a91-b75108b756c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 由于没有现成的文件，所以我们只能构造一个\n",
    "df = spark.read.csv(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.selectExpr(\"`Order Number` as OrderNumber\",\"`Order Date` as OrderDate\",\"`Item Name` as ItemName\",\"cast(Quantity as int) as Quantity\",\"cast(`Product Price` as double) as ProductPrice\",\"cast(`Total products` as int) as TotalProducts\").write.mode(\"overwrite\").parquet(\"/mnt/databrickscontainer1/restaurant-1-orders-parquet\")\n",
    "\n",
    "df.selectExpr(\"`Order Number` as OrderNumber\",\"`Order Date` as OrderDate\",\"`Item Name` as ItemName\",\"cast(Quantity as int) as Quantity\",\"cast(`Product Price` as double) as ProductPrice\",\"cast(`Total products` as int) as TotalProducts\").write.mode(\"overwrite\").orc(\"/mnt/databrickscontainer1/restaurant-1-orders-orc\")\n",
    "\n",
    "df.selectExpr(\"`Order Number` as OrderNumber\",\"`Order Date` as OrderDate\",\"`Item Name` as ItemName\",\"cast(Quantity as int) as Quantity\",\"cast(`Product Price` as double) as ProductPrice\",\"cast(`Total products` as int) as TotalProducts\").write.mode(\"overwrite\").format(\"avro\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "12de79e0-0bd6-49d2-a8b3-ccc63165d7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a2502c7-3d22-4a47-9338-cbd8ddae95b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/mnt/databrickscontainer1/restaurant-1-orders-parquet\")\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4fb28474-9b97-42c2-8e5d-0733262301b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 read.format(\"parquet\").load(path) 合并为 read.parquet(path)\n",
    "spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders-parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65458988-6bc8-4f50-b120-563d33895fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6b15c07-11a4-482b-a573-828185867a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"orc\").load(\"/mnt/databrickscontainer1/restaurant-1-orders-orc\")\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3fb7f16f-ca00-4000-92e3-b1dbe2506b9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 read.format(\"orc\").load(path) 合并为 read.orc(path)\n",
    "spark.read.orc(\"/mnt/databrickscontainer1/restaurant-1-orders-orc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfcb6dae-cfcd-4125-bb30-504392ca13fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### avro\n",
    "\n",
    "* 不支持将 read.format(\"avro\").load(path) 合并为 read.avro(path)\n",
    "* 需要使用spark-submit进行提交运行\n",
    "> spark-submit --packages org.apache.spark:spark-avro_2.12:3.2.1 --name readAvro --master yarn readAvro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4366aa2f-b52a-4153-b232-28fb0338c707",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"avro\").load(\"/mnt/databrickscontainer1/restaurant-1-orders-avro\")\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c531c05-e598-4556-8956-d74a40a6c9ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 传统的结构化数据源\n",
    "\n",
    "首先我们创建一批数据。\n",
    "\n",
    "```sql\n",
    "create database spark;\n",
    "use spark;\n",
    "\n",
    "drop table spark_read_test;\n",
    "\n",
    "create table spark_read_test (\n",
    "    Order_Number varchar(10),\n",
    "    Order_Date varchar(20),\n",
    "    Item_Name varchar(30),\n",
    "    Quantity int,\n",
    "    Product_Price decimal(20,2),\n",
    "    Total_products int\n",
    ");\n",
    "\n",
    "insert into spark_read_test\n",
    "values\n",
    "('16089','02/08/2019 18:41','Plain Papadum 5','5','0.8','21'),\n",
    "('15879','20/07/2019 16:55','Plain Papadum 5','12','1.3','7'),\n",
    "('15133','01/06/2019 13:04','Plain Papadum 5','20','0.6','17'),\n",
    "('14752','11/05/2019 17:48','Plain Papadum 5','3','2.5','8'),\n",
    "('13212','02/02/2019 17:47','Plain Papadum 5','13','3.7','38')\n",
    ";\n",
    "\n",
    "select * from spark_read_test;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "345e64bd-f5dc-4b4f-8f64-dca30b59c684",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### jdbc\n",
    "\n",
    "我们可以通过jdbc链接读取数据库中的数据。\n",
    "\n",
    "读取jdbc数据，需要指定一些参数：\n",
    "* url：数据库的链接字符串，如果数据库表中的数据有中文，建议使用 useUnicode=true 来确保传输中不出现乱码\n",
    "* user：连接数据库的用户\n",
    "* password：连接数据库的密码\n",
    "* query：去读数据的查询语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4ded635-9ed5-44c6-af71-a858a0408b07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:mysql://wux-mysql.mysql.database.azure.com:3306/spark?useSSL=true&requireSSL=false\").option(\"user\",\"wux_labs@wux-mysql\").option(\"password\",\"Pa55w.rd\").option(\"query\",\"select * from spark_read_test\").load()\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab2cf9ca-c38b-45f8-947f-5717a6fe2eb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的入门操作\n",
    "\n",
    "DataFrame支持两种风格进行编程，分别是：\n",
    "* DSL风格\n",
    "* SQL风格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c32c0be1-15e7-4734-bb68-11697b285eb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### printSchema\n",
    "\n",
    "功能：打印输出DataFrame的schema信息。\n",
    "\n",
    "语法：  \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f716ca2-0935-4196-b264-27ac32a0ae90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3abeba0a-4ede-48fe-bc2b-f989e61513d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### show\n",
    "\n",
    "功能：展示DataFrame中的数据，默认展示20条。\n",
    "\n",
    "语法：  \n",
    "df.show(参数1, 参数2)\n",
    "* 参数1：默认是20, 控制展示多少条\n",
    "* 参数2：是否截断列的数据，默认只输出20个字符的长度，过长不显示，要显示的话需要指定 truncate = False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "27eaca48-749f-4205-b8f8-0196db1ba711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"text\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\")\n",
    "\n",
    "df.show()\n",
    "df.show(truncate=False)\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08116163-047b-4b2d-af4c-e0e01ac4fed4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DSL语法风格\n",
    "\n",
    "DSL称之为：**领域特定语言**，其实就是指DataFrame的特有API。\n",
    "\n",
    "DSL风格意思就是以调用API的方式来处理数据，比如：df.select().where().limit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc1dc7ee-6cf8-42c0-99db-37cd8fd27c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### select\n",
    "\n",
    "功能：选择DataFrame中的指定列（通过传入参数进行指定）。\n",
    "\n",
    "语法：  \n",
    "df.select(\\*cols)\n",
    "\n",
    "可传递：\n",
    "* 可变参数的cols对象，cols对象可以是Column对象来指定列或者字符串列名来指定列。\n",
    "* List[Column]对象或者List[str]对象，用来选择多个列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b25e8d1-fd29-470f-9b65-294ec7a97519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "print(type(df[\"Order Number\"]))\n",
    "\n",
    "# 使用可变参数的Column对象\n",
    "df.select(df[\"Order Number\"], df[\"Order Date\"]).show()\n",
    "# 使用List[Column]对象\n",
    "df.select([df[\"Order Number\"], df[\"Order Date\"], df[\"Item Name\"]]).show()\n",
    "\n",
    "# 使用可变参数的str对象\n",
    "df.select(\"Order Number\", \"Order Date\").show()\n",
    "# 使用List[str]对象\n",
    "df.select([\"Order Number\", \"Order Date\", \"Item Name\", \"Quantity\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd98bf3b-9853-49ac-98c6-96138b49534e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### selectExpr\n",
    "\n",
    "功能：投影一组 SQL 表达式并返回新的 DataFrame，这是 select 的一个变体，它接受 SQL 表达式。\n",
    "\n",
    "语法：  \n",
    "df.selectExpr(\\*expr)\n",
    "\n",
    "可传递：\n",
    "* 可变参数的SQL表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5910d1f2-02ec-49c2-bf19-a796da6b2556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "# 直接查询字段\n",
    "df.select(\"Quantity\").show(5)\n",
    "# df.select(df[\"Quantity\"]).show(5)\n",
    "\n",
    "# 字段的简单运算，如何实现？\n",
    "df.select(df[\"Quantity\"] + 2).show(5)\n",
    "# df.select(\"Quantity\" + 2).show(5)\n",
    "# df.select(\"Quantity + 2\").show(5)\n",
    "\n",
    "# 字段别名等复杂操作如何实现？\n",
    "df.select(df[\"Quantity\"].alias(\"SHULIANG\")).show(5)\n",
    "# df.select(\"Quantity as SHULIANG\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9fa2b865-11ea-4295-8285-ca6d833ec5b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 直接查询字段\n",
    "# df.select(\"Quantity\").show(5)\n",
    "# df.select(df[\"Quantity\"]).show(5)\n",
    "\n",
    "# 字段的简单运算，如何实现？\n",
    "# df.select(df[\"Quantity\"] + 2).show(5)\n",
    "df.selectExpr(\"Quantity + 2\").show(5)\n",
    "df.selectExpr(\"Quantity\", \"Quantity + 2\").show(5)\n",
    "\n",
    "# 字段别名等复杂操作如何实现？\n",
    "# df.select(df[\"Quantity\"].alias(\"SHULIANG\")).show(5)\n",
    "df.selectExpr(\"Quantity as SHULIANG\").show(5)\n",
    "df.selectExpr(\"Quantity\", \"Quantity as SHULIANG\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39354c12-6db3-40b4-bbcb-1bfb3180a0e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### distinct\n",
    "\n",
    "功能：返回不包含重复数据的新的DataFrame。\n",
    "\n",
    "语法：  \n",
    "df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71df4c71-5f4d-48bc-a0ec-55a3ee6c333b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.select(df[\"Order Number\"]).show()\n",
    "\n",
    "df.select(df[\"Order Number\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e00b0cd-dc99-4853-bb10-e5371701d7f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### filter和where\n",
    "\n",
    "功能：过滤DataFrame内的数据，返回一个过滤后的DataFrame。\n",
    "\n",
    "语法：  \n",
    "df.filter()  \n",
    "df.where()\n",
    "\n",
    "> where和filter在功能上是等价的。  \n",
    "> :func:`where` is an alias for :func:`filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c8987ea-0341-4f79-88df-b9be04463902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.filter(\"Quantity > 30\").show()\n",
    "df.filter(df[\"Quantity\"] > 30).show()\n",
    "\n",
    "df.where(\"Quantity > 30\").show()\n",
    "df.where(df[\"Quantity\"] > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b5c24b9-7001-4151-b8d8-f329e7145f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### groupBy\n",
    "\n",
    "功能：按照指定的列进行数据的分组，返回值是**GroupedData对象**。\n",
    "\n",
    "语法：  \n",
    "df.groupBy()\n",
    "\n",
    "> 传入参数和select一样，支持多种形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb335b7e-60b5-408a-9476-3c9a623ee13d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.groupBy(\"Quantity\").count().show()\n",
    "df.groupBy(\"Item Name\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ca06df9-7395-42f4-94f8-c85531fb9799",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### GroupedData对象\n",
    "\n",
    "GroupedData对象是一个特殊的DataFrame数据集，其类全名：<class 'pyspark.sql.group.GroupedData'>。\n",
    "\n",
    "这个对象是经过groupBy后得到的返回值，内部记录了以分组形式存储的数据。\n",
    "\n",
    "GroupedData对象有很多API，比如前面的count方法就是这个对象的内置方法，除此之外，像：min、max、avg、sum、等等许多方法都存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f0ac64b-dae8-4cfd-b470-d566a9662b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# 类全名：<class 'pyspark.sql.group.GroupedData'>\n",
    "print(type(df.groupBy(\"Quantity\")))\n",
    "\n",
    "df.groupBy(\"Quantity\").count().show()\n",
    "df.groupBy(\"Item Name\").min().show()\n",
    "df.groupBy(\"Item Name\").max().show()\n",
    "df.groupBy(\"Item Name\").sum().show()\n",
    "df.groupBy(\"Item Name\").avg().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11fe7a56-5cf3-4df1-a09e-b29f9914e9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.groupBy(\"Quantity\").count().show()\n",
    "df.groupBy(\"ItemName\").count().show()\n",
    "\n",
    "df.groupBy(\"ItemName\").min(\"Quantity\").show()\n",
    "df.groupBy(\"ItemName\").max(\"Quantity\").show()\n",
    "df.groupBy(\"ItemName\").sum(\"Quantity\").show()\n",
    "df.groupBy(\"ItemName\").avg(\"Quantity\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ffbc5e86-d686-4736-94d1-1d66db0cb811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### sort和orderBy\n",
    "\n",
    "功能：对DataFrame的数据进行排序。\n",
    "\n",
    "语法：  \n",
    "df.sort(\\*cols, \\*kwargs)  \n",
    "* cols：排序的列。\n",
    "* kwargs：排序参数，用于指定是顺序排序还是倒序排序，接受boolean或List(boolean)。如果是List，则元素个数应该与cols的元素个数相同。\n",
    "\n",
    "> orderBy = sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71ce97ec-538d-4146-af20-725774f6d60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54efb005-4345-43eb-87da-cca313edb09a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sort(\"ItemName\").show()\n",
    "df.sort(df[\"ItemName\"], ascending=False).show()\n",
    "df.sort([\"ItemName\",\"TotalProducts\"], ascending=[False,False]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "efff7d15-6875-4549-a7b3-68dec3f5f435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.orderBy(\"ItemName\").show()\n",
    "df.orderBy(df[\"ItemName\"], ascending=False).show()\n",
    "df.orderBy([\"ItemName\",\"TotalProducts\"], ascending=[False,False]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7cb08e80-42ce-468c-8e0a-93defb552064",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### limit\n",
    "\n",
    "功能：限制返回的记录数。\n",
    "\n",
    "语法：  \n",
    "df.limit(num)\n",
    "* num：指定限制返回的记录数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f2d15f24-7be0-4bbe-a262-4f1250276e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.show() # 20\n",
    "df.show(5) # 5\n",
    "\n",
    "df.limit(5).show() # 20 -> 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "594c01f6-4859-4a6d-9e28-51c1741f1985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### alias\n",
    "\n",
    "功能：返回一个带有别名的DataFrame，并且可以根据别名访问DataFrame中的数据。\n",
    "\n",
    "语法：  \n",
    "df.alias(alias)\n",
    "* alias：指定别名的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a2fc4d3-27ad-43a2-a5a4-396af2a520a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df2 = df.alias(\"df_name\")\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "df2.select(\"ItemName\").show()\n",
    "df2.select(\"df_name.ItemName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49989487-5825-4d82-9888-d13b801c1d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### join\n",
    "\n",
    "功能：将两个DataFrame进行关联操作返回新的DataFrame。\n",
    "\n",
    "语法：  \n",
    "df.join(other, on=None, how=None)\n",
    "* other：与df进行join操作的另一个DataFrame\n",
    "* on：join操作的关联条件\n",
    "* how：join操作的关联方式，支持 inner、outer、cross、full、fullouter、left、leftouter、right、rightouter、semi、anti、...\n",
    "  * 默认是 inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1a253e9-e816-4910-b918-0b6fd56497fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df1 = df.where(\"OrderNumber in ('16116','16117')\").alias(\"df1_name\")\n",
    "df2 = df.where(\"OrderNumber in ('16117','16118')\").alias(\"df2_name\")\n",
    "\n",
    "display(df1.join(df2))\n",
    "display(df1.join(df2,\"OrderNumber\"))\n",
    "display(df1.join(df2,[\"OrderNumber\",\"ItemName\"]))\n",
    "display(df1.join(df2,[\"OrderNumber\",\"ItemName\"],\"left\"))\n",
    "display(df1.join(df2,[col(\"df1_name.OrderNumber\") == col(\"df2_name.OrderNumber\"),col(\"df1_name.ItemName\") == col(\"df2_name.ItemName\")],\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "108bb55c-6d60-42fb-963e-4a33b05303ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SQL语法风格\n",
    "\n",
    "SQL风格就是使用SQL语句处理DataFrame的数据，比如：spark.sql(\"SELECT * FROM xxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b96d0bd5-2773-4116-860e-a87cf03d2978",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 注册DataFrame成为表\n",
    "\n",
    "DataFrame的一个强大之处就是我们可以将它看作是一个关系型数据表，然后可以通过在程序中使用spark.sql()来执行SQL语句查询，结果返回一个DataFrame。\n",
    "\n",
    "如果想使用SQL风格的语法，需要将DataFrame注册成表，采用如下的方式：\n",
    "```\n",
    "df.createTempView()                注册一个临时表，如果表已存在则报错\n",
    "df.createOrReplaceTempView()       注册一个临时表，如果存在则进行替换\n",
    "df.createGlobalTempView()          注册一个全局表，如果表已存在则报错\n",
    "df.createOrReplaceGlobalTempView() 注册一个全局表，如果存在则进行替换\n",
    "```\n",
    "临时表只能在当前SparkSession中使用；全局表（Global）可以跨SparkSession使用，使用时需要用global_temp做前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bee8a322-1ee1-4be8-8c43-8479d6f99ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "# df.createTempView(\"temp_orders\")\n",
    "# Temporary view 'temp_orders' already exists\n",
    "# df.createTempView(\"temp_orders\")\n",
    "df.createOrReplaceTempView(\"temp_orders\")\n",
    "# df.createGlobalTempView(\"temp_orders_global\")\n",
    "df.createOrReplaceGlobalTempView(\"temp_orders_global\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54b1d7a6-c446-4c13-adf8-c0f72c7a3f7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "注册好表后，可以通过spark.sql(sql)来执行sql查询，返回一个新的DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "724cb9ef-fbf5-4778-b417-c05cef4f80b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from temp_orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6b50c257-8587-4809-a7de-f9c356f3cc56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from global_temp.temp_orders_global\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44f4db16-e9f5-4016-bf75-a26f6de09397",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from temp_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f43c0ac5-a778-43dd-8966-6239070d3f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.createTempView(\"orders\")\n",
    "\n",
    "spark.sql(\"select ItemName,Quantity from orders\").show()\n",
    "spark.sql(\"select ItemName,max(Quantity),min(Quantity),sum(Quantity) from orders group by ItemName\").show()\n",
    "spark.sql(\"select ItemName,max(Quantity) as MaxQuantity,min(Quantity) as MinQuantity,sum(Quantity) from orders as t1 group by ItemName\").show()\n",
    "spark.sql(\"select distinct ItemName,Quantity,Quantity+5,Quantity*5 from orders where Quantity > 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "132203f8-3cba-494a-be6a-a795ef31945c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d9f9d73-f739-4337-a775-0e0551e6e658",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 内置函数\n",
    "\n",
    "PySpark提供了一个包：pyspark.sql.functions。这个包里面提供了一系列的计算函数供SparkSQL使用。\n",
    "\n",
    "使用之前需要先导入相关的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91969aaf-2642-4a6a-ac06-2229d7c9ae65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd93f190-ba2f-4f02-82ea-b53da4978a80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 统计函数\n",
    "\n",
    "**count、max、min、sum、avg、mean**\n",
    "* count：统计数量\n",
    "* max：取最大值\n",
    "* min：取最小值\n",
    "* sum：求和\n",
    "* avg：求均值\n",
    "* mean：求均值\n",
    "\n",
    "说明：\n",
    "* 这些函数都接收一个Column作为参数\n",
    "* 这些函数都返回一个Column\n",
    "* avg和mean效果是一样的，都是avg\n",
    "* 返回的是一个聚合值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68f92bc8-d258-4f78-8533-78d539890782",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df.select(F.count(\"TotalProducts\"), F.max(\"TotalProducts\"), F.min(\"TotalProducts\"), F.sum(\"TotalProducts\"),\n",
    "          F.avg(\"TotalProducts\"), F.mean(\"TotalProducts\")).show()\n",
    "\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "spark.sql(\"select count(TotalProducts),max(TotalProducts),min(TotalProducts),sum(TotalProducts),avg(TotalProducts),mean(TotalProducts) from orders\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a810e73-c759-42ca-80df-455091b7bc0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 简单计算函数\n",
    "\n",
    "**abs、exp、sqrt、sin、cos、rand、round、ceil、floor、cbrt**\n",
    "* abs：取绝对值\n",
    "* exp：取指数\n",
    "* sqrt：开平方\n",
    "* sin：正弦值\n",
    "* cos：余弦值\n",
    "* rand：生成随机数\n",
    "* round：四舍五入\n",
    "* ceil：向上取整\n",
    "* floor：向下取整\n",
    "* cbrt：开三次方\n",
    "\n",
    "说明：\n",
    "* 这些函数都接收一个Column作为参数(rand不需要参数)\n",
    "* 这些函数都返回一个Column\n",
    "* 返回的是一个单值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff455911-aac5-4e7a-b095-3bfd93f333d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "display(df.select(F.abs(\"TotalProducts\"), F.exp(\"TotalProducts\"), F.sqrt(\"TotalProducts\"), F.sin(\"TotalProducts\"), F.abs(F.sin(\"TotalProducts\")), F.cos(\"TotalProducts\"), F.abs(F.cos(\"TotalProducts\")), F.rand()))\n",
    "\n",
    "display(df.select(F.round(\"TotalProducts\"), F.round(F.cos(\"TotalProducts\")), F.ceil(\"TotalProducts\"), F.ceil(F.cos(\"TotalProducts\")), F.floor(\"TotalProducts\"), F.floor(F.cos(\"TotalProducts\")), F.cbrt(\"TotalProducts\")))\n",
    "\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "display(spark.sql(\"select abs(TotalProducts),exp(TotalProducts),sqrt(TotalProducts),sin(TotalProducts),cos(TotalProducts),abs(cos(TotalProducts)),rand() from orders\"))\n",
    "display(spark.sql(\"select round(TotalProducts),round(cos(TotalProducts)),ceil(TotalProducts),ceil(cos(TotalProducts)),floor(TotalProducts),floor(cos(TotalProducts)),cbrt(TotalProducts) from orders\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "602649fb-5d97-4cdf-b5db-fb580b7a17e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 时间相关的函数\n",
    "\n",
    "**current_date、current_timestamp、date_add、date_sub、datediff、dayofmonth、dayofweek、dayofyear、last_day、year、month**\n",
    "* current_date：返回当前日期\n",
    "* current_timestamp：返回当前时间戳\n",
    "* date_add：返回指定日期加几天的日期\n",
    "* date_sub：返回指定日期减几天的日期\n",
    "* datediff：返回两个日期相差的天数\n",
    "* dayofmonth：返回指定日期在所在月的第几天\n",
    "* dayofweek：返回指定日期在所在周的第几天\n",
    "* dayofyear：返回指定日期在所在年的第几天\n",
    "* last_day：返回指定日期在所在月的最后一天\n",
    "* year：返回指定日期所在的年\n",
    "* month：返回指定日期所在的月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97244b25-34a0-4858-b85f-1bd1ef90d38f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "# current_date、current_timestamp、date_add、date_sub、datediff、dayofmonth、dayofweek、dayofyear、last_day、year、month\n",
    "display(df.select(F.current_date(), F.current_timestamp(), F.date_add(F.current_date(), 5), F.date_sub(F.current_date(), 5), F.datediff(F.date_add(F.current_date(), 5), F.date_sub(F.current_date(), 5)), F.dayofmonth(F.current_date()), F.dayofweek(F.current_date()), F.dayofyear(F.current_date()), F.last_day(F.current_date()), F.year(F.current_date()), F.month(F.current_date())))\n",
    "\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "display(spark.sql(\"select current_date(), current_timestamp(), date_add(current_date(), 5), date_sub(current_date(), 5), datediff(date_add(current_date(), 5), date_sub(current_date(), 5)), dayofmonth(current_date()), dayofweek(current_date()), dayofyear(current_date()), last_day(current_date()), year(current_date()), month(current_date()) from orders\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ac1d65b-5c25-4543-8998-81a295f9ef3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 炸裂函数\n",
    "\n",
    "**explode**\n",
    "* explode(expr)：通过取消嵌套 expr 返回行。expr：一个 ARRAY 或 MAP 或表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea946c0a-d885-41dc-88a7-0089bd6f6915",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.where(\"OrderNumber = 16118\").createOrReplaceTempView(\"orders\")\n",
    "\n",
    "display(spark.sql(\"select * from orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5d0d291-8aba-4f79-aeb2-dafb26f34575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 将Array数据炸裂成多行\n",
    "select explode(array(10,20,30,35)),t.* from orders t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f93207f2-eb12-43e1-a613-b16c5e0519d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 将Map数据炸裂成多行\n",
    "select explode(map(10,'10a',20,'20a',30,'30a')),t.* from orders t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f7e2555-442c-4d1f-9f84-d5e8178302fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 根据表达式结果炸裂成多行，需要表达式的返回结果是Array或Map\n",
    "select explode(split(ItemName, \" \")),t.* from orders t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "001d694d-7652-41e7-ad3b-be607bbf1ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 判断相关的函数\n",
    "\n",
    "**is distinct from、isfalse、isnan、isnotnull、isnull、istrue、forall**\n",
    "* is distinct from：判断两个表达式是否具有不同的值\n",
    "* isfalse：判断表达式的值是否是false\n",
    "* isnan：判断表达式是否是 NaN\n",
    "* isnotnull：判断表达式是否非空，等效于 is not null\n",
    "* isnull：判断表达式是否是空，等效于 is null\n",
    "* istrue：判断表达式的值是否是true\n",
    "* forall：判断表达式是否对数组中的所有元素都有效\n",
    "\n",
    "> 由于判断中包含对空值、NaN等的判断，所以我们切换一下数据集，不使用`restaurant-1-orders`，而是使用`泰坦尼克号数据集`进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b05ed54-97dd-46ef-86e1-93743d971fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/mnt/databrickscontainer1/taitanic_train.csv\", header=True)\n",
    "\n",
    "df.createOrReplaceTempView(\"taitanic\")\n",
    "\n",
    "# 先看一下原始数据\n",
    "display(spark.sql(\"select * from taitanic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b3591b2-3e97-40fe-9d6f-0d20dfc21f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select age, case when age is null then 'NaN' else age end as age1, age is distinct from 22, age is not distinct from 22, isnan(age), isnan(cast(case when age is null then 'NaN' else age end as double)) as age_nan,isnotnull(age), isnull(age), embarked, forall(array(embarked), x -> x in ('S','C','Q')) as for1, forall(array(embarked), x -> x in ('S','C')) as for2 from taitanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53da1ad7-c475-4f5c-8d03-8cbb58c1891e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 窗口函数\n",
    "\n",
    "**窗口：** 可以理解为满足指定条件的数据记录的集合。\n",
    "\n",
    "**窗口函数：** 在满足指定条件的记录集合（一个窗口）上执行的特殊函数，对于每条记录都要在此窗口内执行函数。\n",
    "\n",
    "**静态窗口：** 有的窗口函数的窗口大小是固定的，这种属于静态窗口。\n",
    "\n",
    "**滑动窗口：** 有的窗口函数的窗口大小是不固定的，这种属于滑动窗口。\n",
    "\n",
    "窗口函数最明显的特征就是带有over()关键字。\n",
    "\n",
    "窗口函数使用order by关键字对数据进行排序，相同数据属于一个窗口；同时可以使用partition by进行数据分区，不同分区的数据独立进行窗口函数的调用。\n",
    "\n",
    "窗口函数同时具备了普通语句的group by功能（用partition by实现的），也具备普通语句的order by功能（用order by实现的）。但是与普通group by不同的是：group by语句会将相同组的数据聚合成一条数据，属于多对一的关系；而partition by在分组之后会保留原有数据的所有记录，不会聚合成一条，属于一对一的关系。\n",
    "\n",
    "窗口函数的引入是为了**既显示聚合前的数据，又显示聚合后的结果**，只是在窗口函数所在的查询位置上添加一列来展示聚合结果。\n",
    "\n",
    "普通聚合函数也可以用于窗口函数中，赋予它窗口函数的功能。由于普通聚合函数只返回一个值，但是窗口函数返回的是多行记录，所以一个窗口中所有记录的聚合结果列的值都是一样的。\n",
    "\n",
    "**row_number() over()、rank() over()、dense_rank() over()、ntile() over()、avgs() over()**\n",
    "* 排序类型\n",
    "  * row_number：返回行号\n",
    "  * rank：返回分组排序号，序号可能不连续\n",
    "  * dense_rank：返回分组排序号，序号连续\n",
    "* 聚合类型\n",
    "  * sum\n",
    "  * count\n",
    "  * avg\n",
    "  * max\n",
    "  * min\n",
    "* 分区类型\n",
    "  * ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fa8d856-10eb-4982-9c9e-bf69323b1adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# 先看一下原始数据\n",
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2fe2a52d-edd3-434f-8808-2da584e5f477",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 排序类型窗口函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8e5fcaad-326f-4bd0-830d-1cc0c91f301a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 对全表按“订单日期”字段排序，返回行号\n",
    "select *,  row_number() over(order by OrderDate desc) as rn from orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f8be857-6d18-4ffe-88e5-67252ecbf717",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 按“项目名称”进行分组，按“订单日期”字段排序，返回分组行号\n",
    "select *,  row_number() over(partition by ItemName order by cast(OrderDate as date) desc) as rn2 from orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d93ed65a-af49-47df-aa17-c529ffe3dcf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 对全表按“订单日期”字段排序，返回分组排序\n",
    "select *, rank() over(order by OrderDate) as rn1, dense_rank() over(order by OrderDate) as rn2 from orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3705c68-944b-428c-884a-86e1ab024e69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 按“项目名称”进行分组，按“数量”字段排序，返回分组排序\n",
    "select *, rank() over(partition by ItemName order by Quantity) as rn1, dense_rank() over(partition by ItemName order by Quantity) as rn2 from orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61391e75-8bdb-4979-b3f2-93ccc45c0e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 聚合类型窗口函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dc8ac1b-82fa-4171-b80a-c6489b38433d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 对全表按“订单日期”字段排序，对产品价格进行聚合\n",
    "select *, sum(cast(ProductPrice as double)) over(order by OrderDate) as s, count(cast(ProductPrice as double)) over(order by OrderDate) as c, avg(cast(ProductPrice as double)) over(order by OrderDate) as a from orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aae47064-379a-4a90-ba8c-36e79c494378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 按“项目名称”进行分组，按“数量”字段排序，对产品价格进行聚合\n",
    "select *, sum(cast(ProductPrice as double)) over(partition by ItemName order by Quantity) as s, count(cast(ProductPrice as double)) over(partition by ItemName order by Quantity) as c, avg(cast(ProductPrice as double)) over(partition by ItemName order by Quantity) as a from orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5199a94c-3dbf-40f8-9357-acfb9fe5e671",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 自定义函数\n",
    "\n",
    "无论是Hive还是SparkSQL在做数据分析处理时，往往需要使用函数。前面我们已经了解了SparkSQL在pyspark.sql.functions中已经提供了很多实现公共功能的函数，那如果我们需要按照自己的需求来设计函数该怎么办呢？SparkSQL与Hive一样支持自定义函数：UDF和UDAF，尤其时UDF在项目中使用最广泛。\n",
    "\n",
    "Hive支持三种自定义函数：\n",
    "* UDF(User-Defined Function)，函数\n",
    "  * 一对一关系，输入一个值经过函数以后输出一个值\n",
    "* UDAF(User-Defined Aggregation Function)，聚合函数\n",
    "  * 多对一关系，输入多行值输出一个值，通常与group by联合使用\n",
    "* UDTF(User-Defined Table-Generation Function)\n",
    "  * 一对多关系，输入一个值输出多行值\n",
    "\n",
    "在SparkSQL中，目前仅支持UDF函数和UDAF函数，而Python仅支持UDF函数。\n",
    "\n",
    "在SparkSQL中定义自定义函数有两种方式：\n",
    "* sparksession.udf.register(参数1, 参数2, 参数3)\n",
    "  * 注册的UDF可以用于DSL和SQL风格，其中返回值用于DSL风格，参数内的名字用于SQL风格\n",
    "  * 参数1：UDF名称，用于SQL风格调用\n",
    "  * 参数2：自定义的函数的函数名称\n",
    "  * 参数3：声明UDF的返回值类型\n",
    "* pyspark.sql.functions.udf(参数1, 参数2)\n",
    "  * 仅能用于DSL风格\n",
    "  * 参数1：自定义的函数的函数名称\n",
    "  * 参数2：声明UDF的返回值类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec6edd55-2b40-4563-9d54-985030352b86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### UDF\n",
    "\n",
    "下面我们自定义一个函数，让价格加上5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c2fc539-e3bd-40de-847c-7b42e03c8c23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个将价格加5的函数\n",
    "def price_add_5(data):\n",
    "    return float(data) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9cdb757c-0727-40b1-8c8e-fb4922aa68c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 通过方式1进行函数的注册\n",
    "# 参数1：\"udf1\"，用于SQL风格的函数调用\n",
    "# 参数2：price_add_5，自定义函数的函数名称\n",
    "# 参数3：DoubleType()，声明UDF的返回值类型\n",
    "# 返回值：udf2，用于DSL风格的函数调用\n",
    "udf2 = spark.udf.register(\"udf1\", price_add_5, DoubleType())\n",
    "\n",
    "# 通过方式2进行函数的注册\n",
    "# 参数1：price_add_5，自定义函数的函数名称\n",
    "# 参数2：DoubleType()，声明UDF的返回值类型\n",
    "# 返回值：udf3，用于DSL风格的函数调用\n",
    "udf3 = F.udf(price_add_5, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1192cff-b86a-480e-86d8-acd08fa3e8aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# 先看一下原始数据\n",
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7d99e8e9-e14f-4697-a09a-426912e24277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 通过方式1定义的UDF的参数1可以用于SQL风格\n",
    "select t.*, udf1(ProductPrice) from orders t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b501cb69-fc21-47d3-95d1-d3d8a895812f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 通过方式1定义的UDF的返回值可以用于DSL风格\n",
    "df.select(\"ProductPrice\", udf2(\"ProductPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0994e694-51aa-4e8a-ae68-5e180d47d576",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 通过方式2定义的UDF的返回值可以用于DSL风格\n",
    "df.select(\"ProductPrice\", udf3(\"ProductPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "848e9e8e-7a77-4652-920b-0d3343d9e139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 但是，两种定义方式的返回值对应的函数名称，不可以用于SQL风格\n",
    "select t.*, udf2(ProductPrice), udf3(ProductPrice) from orders t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a31b2e29-c59f-4c0d-97ab-38d37c5b5f17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### UDAF\n",
    "\n",
    "Python并不支持UDAF，要实现UDAF需要使用Scala或Java进行代码编写。\n",
    "\n",
    "SparkSQL提供了内置函数 avg/mean，计算逻辑是：![](https://www.zhihu.com/equation?tex=Avg%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En+x_i%7Dn)。\n",
    "\n",
    "我们自定义一个函数，求分组中最大值与最小值的平均值：![](https://www.zhihu.com/equation?tex=MyAvg%3D%5Cfrac%7Bmax%28x_i%29%2Bmin%28x_i%29%7D2)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6be29214-9ad9-4c4a-abf9-25a6d8ffef30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import scala.math.{max, min}\n",
    "import org.apache.spark.sql.{Encoder, Encoders}\n",
    "import org.apache.spark.sql.expressions.Aggregator\n",
    "\n",
    "case class Average(var min: Double, var max: Double)\n",
    "\n",
    "object MyAggregator extends Aggregator[Double, Average, Double] {\n",
    "  \n",
    "  def bufferEncoder: org.apache.spark.sql.Encoder[Average] = Encoders.product\n",
    "  def outputEncoder: org.apache.spark.sql.Encoder[Double] = Encoders.scalaDouble\n",
    "  \n",
    "  // 初始化缓存数据\n",
    "  def zero: Average = Average(Double.MaxValue, Double.MinValue)\n",
    "  \n",
    "  // 相同分区下，对下一个值进行聚合的逻辑\n",
    "  // 缓存数据用于存放最大值、最小值\n",
    "  def reduce(buffer: Average, value: Double): Average = Average(min(buffer.min, value), max(buffer.max, value))\n",
    "  \n",
    "  // 不同分区之间的数据合并\n",
    "  def merge(buffer1: Average,buffer2: Average): Average = Average(min(buffer1.min, buffer2.min), max(buffer1.max, buffer2.max))\n",
    "  \n",
    "  // 聚合结束后，返回什么\n",
    "  // 返回最大值、最小值的平均值 (reduction.min + reduction.max) / 2\n",
    "  def finish(reduction: Average): Double = (reduction.min + reduction.max) / 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "737bf04d-c2ca-4162-a531-262063959a7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.udf.register(\"MyAvg\",udaf(MyAggregator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f953918-0a28-4012-bbb3-fbc06e9bf0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// 由于单个产品的价格是相同的，所以取最大值、最小值的结果是一样的，求平均就没什么意义\n",
    "// 所以我们将单价乘以数量，得到的是每个订单的购买总价，由于订单中的数量不同，所以总价就不同，可以看出效果\n",
    "spark.sql(\"select ItemName, avg(Quantity * ProductPrice) as avg, max(Quantity * ProductPrice) as max, min(Quantity * ProductPrice) as min, MyAvg(Quantity * ProductPrice) as myavg from orders group by ItemName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9e071f3-d3aa-43f2-b59e-0fb89c4b1425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的数据清洗\n",
    "\n",
    "上面我们处理的数据实际上都是已经被处理好的规整数据，但在实际生产中，数据可能是杂乱无章的，这就需要我们先对数据进行清洗，整理为符合处理要求的规整数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "050179fa-3a74-4b88-adaa-617f92b1931f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 删除重复行\n",
    "\n",
    "**dropDuplicates()**\n",
    "\n",
    "功能：对DataFrame的数据进行去重，如果重复数据有多条，取第一条。\n",
    "\n",
    "语法：  \n",
    "df.dropDuplicates(subset=None)  \n",
    "* subset：去重的字段子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d161a82-0c8c-41db-9ae6-a1d44c5b9464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# |Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "# 构建一个包含重复数据的DataFrame\n",
    "dfDuplicates = df.select(df[\"Order Number\"], df[\"Item Name\"], df[\"Quantity\"], df[\"Total products\"])\n",
    "\n",
    "# 缓存一下数据\n",
    "dfDuplicates.cache()\n",
    "\n",
    "# 展示原始记录数\n",
    "print(dfDuplicates.count())\n",
    "\n",
    "# 展示有重复的数据\n",
    "dfDuplicates.groupBy(df[\"Order Number\"], df[\"Item Name\"], df[\"Quantity\"], df[\"Total products\"]).count().where(\"count > 1\").show()\n",
    "\n",
    "# 去除重复行\n",
    "dfDistinct = dfDuplicates.dropDuplicates()\n",
    "\n",
    "# 缓存一下\n",
    "dfDistinct.cache()\n",
    "\n",
    "# 展示去重后的记录数\n",
    "print(dfDistinct.count())\n",
    "\n",
    "# 无重复的数据\n",
    "dfDistinct.groupBy(df[\"Order Number\"], df[\"Item Name\"], df[\"Quantity\"], df[\"Total products\"]).count().where(\"count > 1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea6b4ae5-ec4d-4154-8ee0-299375ab461f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dfDuplicates.groupBy(df[\"Order Number\"], df[\"Item Name\"], df[\"Quantity\"], df[\"Total products\"]).count().where(\"count > 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f67d9eb-323e-4905-8128-19a6a5a5797d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "去重可以指定字段，重复的数据保留第一条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b10bec23-36b3-4a90-8c8e-40dc1bea16ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# |Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", header=True)\n",
    "\n",
    "# use '&' for 'and', '|' for 'or', '~' for 'not'\n",
    "dfTest = df.where(((df[\"Order Number\"] == \"7193\") | (df[\"Order Number\"] == \"9360\")) & (df[\"Item Name\"] == \"Bhuna\"))\n",
    "\n",
    "dfTest.show()\n",
    "\n",
    "dfTest.dropDuplicates([\"Item Name\"]).show()\n",
    "dfTest.dropDuplicates([\"Quantity\"]).show()\n",
    "dfTest.dropDuplicates([\"Item Name\", \"Order Date\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "988c1afc-b10c-42cb-8856-1b47fdfeb72d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "display(df.select(\"OrderNumber\",\"OrderDate\"))\n",
    "display(df.select(\"OrderNumber\",\"OrderDate\").dropDuplicates())\n",
    "\n",
    "display(df.orderBy(\"OrderNumber\",\"ItemName\"))\n",
    "display(df.orderBy(\"OrderNumber\",\"ItemName\").dropDuplicates([\"OrderNumber\"]))\n",
    "display(df.where(\"OrderNumber = 10001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb5df23b-a1b9-4541-8e23-6c1904a00718",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 删除有缺失值的行\n",
    "\n",
    "**dropna()**\n",
    "\n",
    "功能：如果数据中包含null，通过dropna来判断，符合条件就删除这一行数据。\n",
    "\n",
    "语法：  \n",
    "df.dropna(how, thresh, subset)  \n",
    "* how：删除方式，\"any\" 表示只要有字段为 null 就删除，\"all\" 表示该行所有字段都为 null 才删除。默认值是any  \n",
    "* thresh：指定阈值进行删除，表示最少有多少列包含有效数据，该行数据才保留  \n",
    "* subset：指定字段列表，表示需要在这些字段中满足上面的条件，该行数据才保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3353d65-a10d-4d26-b071-f419d12245e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/taitanic_train.csv\", header=True)\n",
    "\n",
    "# 展示原始数据\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e578bf1a-eedf-4f3a-b227-ea3fcd750e60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 只有所有字段都是null才删除\n",
    "display(df.dropna(\"all\"))\n",
    "\n",
    "# 默认值any，只要有字段为null就删除\n",
    "display(df.dropna())\n",
    "\n",
    "# 指定需要有3列是有效数据的才保留，由于泰坦尼克号数据集有12个字段，都满足3个有效字段，所以不会删除数据\n",
    "display(df.dropna(thresh=3))\n",
    "\n",
    "# 泰坦尼克号数据中有空值的字段是 \"Age\", \"Cabin\", \"Embarked\"\n",
    "# 指定4个字段中有3个有效则保留，则会允许一个字段为null的数据留下来\n",
    "display(df.dropna(thresh=3, subset=[\"Sex\", \"Age\", \"Cabin\", \"Embarked\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c5cc8dd-0346-4f1e-999e-550a5f6171b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 删除列\n",
    "\n",
    "**drop()**\n",
    "\n",
    "功能：删除指定的列，返回一个不包含指定列的新的DataFrame。\n",
    "\n",
    "语法：  \n",
    "df.drop(*cols) \n",
    "* cols：指定的需要删除的列\n",
    "\n",
    "> 该功能可以用 df.select(指定列之外的列) 来实现。  \n",
    "> 但是当DataFrame的列太多的时候，用select方式需要书写的列名太长，用drop就比较方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa9177cc-e707-4ef3-9b3c-7bf2d9ba0aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/taitanic_train.csv\", header=True)\n",
    "\n",
    "# 展示原始数据\n",
    "display(df)\n",
    "\n",
    "# 删除指定的列\n",
    "display(df.drop(df[\"Cabin\"]))\n",
    "\n",
    "# 可以用select实现，只是代码书写量大\n",
    "display(df.select(\"PassengerId\",\"Survived\",\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Embarked\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0e94623-3353-44bb-ae16-dc69de2f6133",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 填充缺失值\n",
    "\n",
    "**fillna()**\n",
    "\n",
    "如果缺失的数据比较少，删除行或列造成数据丢失比较多，可以使用填充缺失值的方法来规整数据。\n",
    "\n",
    "功能：根据参数规则来填充null值。\n",
    "\n",
    "语法：  \n",
    "df.fillna(value, subset)  \n",
    "* value：填充的值，或者规则\n",
    "* subset：指定填充的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7f390ff-d408-4d58-8366-183df8ee1331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"/mnt/databrickscontainer1/taitanic_train.csv\", header=True)\n",
    "\n",
    "# 展示原始数据\n",
    "display(df)\n",
    "\n",
    "# 对所有缺失值填充指定的值\n",
    "display(df.fillna(\"loss\"))\n",
    "\n",
    "# 对指定的列填充缺失值\n",
    "display(df.fillna(\"loss\", subset=[\"Cabin\"]))\n",
    "\n",
    "# 按照指定规则进行缺失值填充\n",
    "display(df.fillna({\"Age\": 99, \"Cabin\": \"cabin_loss\", \"Embarked\": \"Z\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c84093e-9bb7-441a-8cb9-ad440ac8df10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的持久化\n",
    "\n",
    "在学习SparkCore的时候，我们知道RDD的数据是过程数据，在下一次要用到RDD的数据的时候，再根据血缘关系，从头重新处理一遍RDD的数据。\n",
    "\n",
    "在SparkSQL模块中，DataFrame的数据也是过程数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8151b822-24a2-4592-8d87-7fbe74b37bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.parquet(\"/mnt/databrickscontainer1/restaurant-1-orders.parquet\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "df2 = spark.sql(\"select OrderNumber, ItemName, current_timestamp() as tm from orders\")\n",
    "\n",
    "df3 = df2.select(\"OrderNumber\",\"ItemName\",\"tm\", F.dayofyear(\"tm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ac27b59-ec00-4967-a6b8-9a2ba05ed5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a43847b-b0a1-4565-b338-eea85c25e3b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "多执行几次上面的代码，可以看到df3的数据一直都在发生变化，可以知道DataFrame的数据也需要从头处理一遍。\n",
    "\n",
    "DataFrame也像RDD一样可以进行数据持久化，同样提供了`cache`、`persist`、`checkpoint`来进行数据的持久化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40b5f17c-4f03-4b81-a3da-28a1bcca95ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8cc4a1c9-0679-4e58-8c2a-d88f4a1acd5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a14f65a3-2535-49a0-89ae-7f457b98808b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "我们将df2或df3进行持久化后，上面的数据就不会发生变化了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aff72ae4-70a8-41b7-9304-5da2fe75ad0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame的数据写出\n",
    "\n",
    "与读取外部数据相对应，可以通过SparkSQL的统一API进行DataFrame数据的写出。\n",
    "\n",
    "支持将数据写出到：\n",
    "* text\n",
    "* csv\n",
    "* json\n",
    "* parquet\n",
    "* orc\n",
    "* avro\n",
    "* jdbc\n",
    "* ...\n",
    "\n",
    "统一API示例代码：\n",
    "```\n",
    "spark.write.mode(\"append|overwrite|ignore|error\").format(\"text|csv|json|parquet|orc|avro|jdbc|......\")\n",
    ".option(\"K\", \"V\") # option可选\n",
    ".save(\"文件的路径, 支持本地文件系统和HDFS\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20defe5b-7cc4-4976-a81b-f2293ab4840d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "schema = StructType().\\\n",
    "add(\"OrderNumber\", StringType(), nullable=False).\\\n",
    "add(\"OrderDate\", StringType(), nullable=False).\\\n",
    "add(\"ItemName\", StringType(), nullable=False).\\\n",
    "add(\"Quantity\", StringType(), nullable=False).\\\n",
    "add(\"ProductPrice\", StringType(), nullable=False).\\\n",
    "add(\"TotalProducts\", StringType(), nullable=False)\n",
    "\n",
    "df = spark.read.csv(\"/mnt/databrickscontainer1/restaurant-1-orders.csv\", schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "569d62d1-a278-4988-8d52-ff32fe60af0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd3825ec-20d1-4dd1-8a5a-706722cb34f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 文本类型数据文件\n",
    "\n",
    "文本类型的数据可以直接用简单的文本编辑器打开进行查看或编辑，比如：text文件、csv文件、json文件等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6943540c-589f-4d2d-a296-af1d2e7b6f77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### text\n",
    "\n",
    "Text data source supports only a single column.\n",
    "\n",
    "写出到文本类型的文件，仅支持一个列，有多余的列则会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33c30f53-6b7b-4ee6-846b-5c1d8dec15c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.where(\"OrderNumber=16118\").select(\"ItemName\").write.mode(\"overwrite\").format(\"text\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6c6d34d-fb21-4571-97a4-b71a46577cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 write.format(\"text\").save(path) 合并为 write.text(path)\n",
    "df.where(\"OrderNumber=16118\").select(\"ItemName\").write.mode(\"overwrite\").text(\"/mnt/databrickscontainer1/restaurant-1-orders-text2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fef49d29-cd6d-4332-b60b-b04c4394864d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### csv & json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0027182-ad8a-4fd4-9c92-f8bd5deea288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.where(\"OrderNumber=16118\").write.mode(\"overwrite\").format(\"csv\").option(\"sep\", \",\").option(\"header\", True).save(\"/mnt/databrickscontainer1/restaurant-1-orders-csv\")\n",
    "\n",
    "df.where(\"OrderNumber=16118\").write.mode(\"overwrite\").format(\"json\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32c29f52-05fb-4d52-895c-083b341abea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 write.format(\"csv\").save(path) 合并为 write.csv(path)\n",
    "df.where(\"OrderNumber=16118\").write.mode(\"overwrite\").csv(\"/mnt/databrickscontainer1/restaurant-1-orders-csv2\", sep=\",\", header=True)\n",
    "# 将 write.format(\"json\").save(path) 合并为 write.json(path)\n",
    "df.where(\"OrderNumber=16118\").write.mode(\"overwrite\").json(\"/mnt/databrickscontainer1/restaurant-1-orders-json2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb648e5a-33b5-4a61-9ab9-1521c3e297fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 自带schema的数据文件\n",
    "\n",
    "在大数据环境中，还有其他各种各样的数据格式，比如：Parquet、Avro、ORC。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33d62f3b-8cb0-4fcf-86fa-a62816b79e00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### parquet & orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f340e6b6-1c00-4f1d-a94f-43ebd0d4a366",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"parquet\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-parquet\")\n",
    "df.write.mode(\"overwrite\").format(\"orc\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca5c5ad0-e0d8-4306-9c3b-e2cd7358d144",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 将 write.format(\"parquet\").save(path) 合并为 write.parquet(path)\n",
    "df.write.mode(\"overwrite\").parquet(\"/mnt/databrickscontainer1/restaurant-1-orders-parquet2\")\n",
    "# 将 write.format(\"orc\").save(path) 合并为 write.orc(path)\n",
    "df.write.mode(\"overwrite\").orc(\"/mnt/databrickscontainer1/restaurant-1-orders-orc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d024e2c-65f8-438a-9526-f5cf8967cfa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### avro\n",
    "\n",
    "avro文件不支持不合格的列名称，比如包含空格：\"Order Number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eba083ed-03fd-4ad8-b07e-cc7da59cb0e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"avro\").save(\"/mnt/databrickscontainer1/restaurant-1-orders-avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "828608ec-0650-4220-9147-adfe905a5559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 传统的结构化数据源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "312dc346-c357-4651-8742-1bea3956bf2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### jdbc\n",
    "\n",
    "我们可以通过jdbc链接将DataFrame的数据写入数据库中。\n",
    "\n",
    "需要指定一些参数：\n",
    "* url：数据库的链接字符串，如果数据库表中的数据有中文，建议使用 useUnicode=true 来确保传输中不出现乱码\n",
    "* user：连接数据库的用户\n",
    "* password：连接数据库的密码\n",
    "* dbtable：指定要写入的表名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de1b8f82-61a5-419d-895a-88aa387fd4cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"jdbc:mysql://wux-mysql.mysql.database.azure.com:3306/spark?useSSL=true&requireSSL=false\").option(\"user\",\"wux_labs@wux-mysql\").option(\"password\",\"Pa55w.rd\").option(\"dbtable\",\"spark_write_test\").option(\"showSql\",True).save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame入门",
   "notebookOrigID": 2180139033493954,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
