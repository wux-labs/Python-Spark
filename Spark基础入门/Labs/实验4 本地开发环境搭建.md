# 实验4 本地开发环境搭建

## PyCharm安装

PyCharm是一种Python IDE（Integrated Development Environment，集成开发环境），带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具，比如调试、语法高亮、项目管理、代码跳转、智能提示、自动完成、单元测试、版本控制等。因此我们使用PyCharm来进行相关程序的开发。

根据自身情况，自行下载[https://www.jetbrains.com/pycharm/download/#section=windows](https://www.jetbrains.com/pycharm/download/#section=windows)相应的版本，完成安装。

## 创建项目并配置运行环境

打开刚安装好的PyCharm，通过点击Projects -> New Project 来进行项目的创建。

![image-20220418092809961](images/image-20220418092809961.png)

选择Pure Python项目，设置Location，在设置Python虚拟环境的地方我们选择新建一个。

![image-20220418093155927](images/image-20220418093155927.png)

前面我们在部署Spark的时候，在Linux系统上配置了Anaconda的Python环境，供Spark集群使用。现在，如果我们要在个人电脑上开发Python Spark程序，也需要配置Python(Anaconda)和PySpark库。我们可以在本机电脑上安装Anaconda、创建虚拟环境、安装PySpark库，然后在这个上面运行我们的开发程序。PySpark支持在Windows上执行，但是会有性能问题以及一些小bug，在Linux上执行是完美和高效的，因此我们还有另外一个选择，既然Linux服务器上已经配置好了环境，我们就可以直接用这个环境，而不必再本机再安装一次。这样做的好处是：开发使用的Python版本、Anaconda版本、虚拟环境、依赖库等都与正式运行的环境保持一模一样，可以解决Linux与Windows版本差异导致的问题；这样做的不足之处是：本机必须与Linux服务器时刻保持连接，一旦断开服务器连接，由于本机没有相关环境，开发将无法进行。

PyCharm为我们提供了直接使用远程Linux服务器上的Python环境的功能。

接上一步，在创建Python环境的时候，选择SSH Interpreter，并填好服务器信息，选择下一步。

![image-20220418094807478](images/image-20220418094807478.png)

根据实际情况设置使用Password或Key pair进行连接，然后下一步。

![image-20220418095027696](images/image-20220418095027696.png)

连接成功后，选择远程服务器上的虚拟环境及python解释器的路径。

![image-20220418095631901](images/image-20220418095631901.png)

设置完成后，点击完成。

![image-20220418100003068](images/image-20220418100003068.png)

最后，由于Python环境在Linux服务器，运行也就在Linux服务器上，但是开发的代码却在本机，所以需要指定一个远程目录，用来将本地项目的代码同步到Linux服务器，也就是Linux上的代码存放路径。完成后点击创建。

![image-20220418100350153](images/image-20220418100350153.png)

这样我们就创建好了开发项目，并且解释器使用的是Linux的。

![image-20220418100811328](images/image-20220418100811328.png)

并且可以看到我们之前安装好的库。

![image-20220418100909367](images/image-20220418100909367.png)

## 开发测试

### Local模式运行

我们开发一段代码来进行环境测试。

```python
from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
    conf = SparkConf().setAppName("EnvTest").setMaster("local[*]")
    sc = SparkContext(conf=conf)
    print(sc.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect())
```

然后，右键运行。

![image-20220418102056321](images/image-20220418102056321.png)

可以看到，运行成功。

![image-20220418102520028](images/image-20220418102520028.png)

### Yarn模式运行

由于我们的代码中设置的master是local[*]，所以，Yarn上无法看到我们提交的应用。

![image-20220418102642532](images/image-20220418102642532.png)

那我们能不能直接提交到Yarn进行运行？答案是可以的。

修改代码，设置master=yarn来试试。

```python
from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
    conf = SparkConf().setAppName("EnvTest").setMaster("yarn")
    sc = SparkContext(conf=conf)
    print(sc.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect())
```

然后右键运行。

![image-20220418103055197](images/image-20220418103055197.png)

代码运行同样是成功的。

![image-20220418103214361](images/image-20220418103214361.png)

此时，我们再看看Yarn的Web UI，可以发现我们的应用提交上去运行了。

![image-20220418103411496](images/image-20220418103411496.png)

###  WordCount案例

接下来我们来看看如何访问HDFS上的文件。

> 由于我们的宿主机跟Docker的环境配置完全一致，并且宿主机可以直接连接Docker容器，所以直接把宿主机当Hadoop的客户端，直接执行相关命令即可。

首先我们创建一个文件`word.txt`

```
Hello World
Hello Python
Hello Spark
Python Spark
PySpark
wux labs pyspark
```

然后将文件上传到HDFS的input目录下。

![image-20220418105853589](images/image-20220418105853589.png)

![image-20220418105944895](images/image-20220418105944895.png)

接下来开发一段WordCount代码。

```python
from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
    print('PySpark WordCount Program')
    # TODO: 当应用运行在集群上的时候，MAIN函数就是Driver Program，必须创建SparkContext对象
    # 创建SparkConf对象，设置应用的配置信息，比如应用名称和应用运行模式
    conf = SparkConf().setAppName("WordCount").setMaster("yarn")
    # TODO: 构建SparkContext上下文实例对象，读取数据和调度Job执行
    sc = SparkContext(conf=conf)
    # 第一步、读取本地数据 封装到RDD集合，认为列表List
    wordsRDD = sc.textFile("hdfs://node1:8020/input/word.txt")
    # 第二步、处理数据 调用RDD中函数，认为调用列表中的函数
    # a. 每行数据分割为单词
    flatMapRDD = wordsRDD.flatMap(lambda line: line.split(" "))
    # b. 转换为二元组，表示每个单词出现一次
    mapRDD = flatMapRDD.map(lambda x: (x, 1))
    # c. 按照Key分组聚合
    resultRDD = mapRDD.reduceByKey(lambda a, b: a + b)
    # 第三步、输出数据
    print(resultRDD.collect())
    # 输出到本地文件中
    resultRDD.saveAsTextFile("hdfs://node1:8020/output/count")
    print('停止 PySpark SparkSession 对象')
    # 关闭SparkContext
    sc.stop()
```

然后运行。

![image-20220418110203630](images/image-20220418110203630.png)

运行完成后，我们可以看到标准输出中打印的结果。

![image-20220418110334345](images/image-20220418110334345.png)

Yarn上可以看到我们提交的应用。

![image-20220418110429330](images/image-20220418110429330.png)

HDSF上可以看到程序的输出结果。

![image-20220418110536013](images/image-20220418110536013.png)

通过命令也可以查看结果。

![image-20220418110727537](images/image-20220418110727537.png)

### 本地调试程序

我们修改一下程序的输出路径，然后给代码打几个断点，右键进行Debug。

![image-20220418111349031](images/image-20220418111349031.png)

可以看到程序在断点处停下来了。

![image-20220418111743198](images/image-20220418111743198.png)

此时的应用还未提交到Yarn执行。

![image-20220418112037092](images/image-20220418112037092.png)

我们执行下一步。

![image-20220418112148529](images/image-20220418112148529.png)

当代码执行到下一步的时候，此时的应用已经提交到Yarn执行。

![image-20220418112307103](images/image-20220418112307103.png)

我们执行到最后一个断点处，此时的标准输出已经输出了。

![image-20220418112448027](images/image-20220418112448027.png)

由于断点语句还没执行，所以HDFS上还没有输出结果。

![image-20220418112525841](images/image-20220418112525841.png)

此时我们可以通过调试窗口，用交互式命令来获取程序的运行状态、结果等信息。

![image-20220418112914457](images/image-20220418112914457.png)

![image-20220418112953040](images/image-20220418112953040.png)

我们还可以看看Spark Web UI。

![image-20220418113757664](images/image-20220418113757664.png)

继续运行断点，下一步，直到程序运行完成，标准输出保留了程序代码中本身的标准输出，还保留了我们调试的时候的交互式输出。

![image-20220418114113119](images/image-20220418114113119.png)

此时HDFS上才有输出结果。

![image-20220418114225258](images/image-20220418114225258.png)

## 总结

安装好Linux服务器上的环境之后，本机开发环境可以不用安装Python，直接使用远程Linux上的环境运行即可。这有几点好处：

* 开发环境与运行环境完全保持一致，可避免Windows与Linux版本差异导致的一些Bug
* 可以直接将本机开发的程序运行在yarn上，直接就是基于yarn环境在进行开发，最大程度的将开发与生产环境保持一致
* 可以直接调试代码，并且是基于yarn运行环境在进行代码调试

当然这种方式也有一些要求：

* 本机需要随时与远程Linux保持连接，一旦连接断开，本地代码无法运行